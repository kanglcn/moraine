# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/API/dl.ipynb.

# %% auto 0
__all__ = ['n2f']

# %% ../nbs/API/dl.ipynb 3
import numpy as np
from numba import prange
import onnxruntime
import importlib
import math
import random
import moraine as mr
from .utils_ import ngjit, ngpjit
from .utils_ import is_cuda_available, get_array_module
from .chunk_ import chunkwise_slicing_mapping
if is_cuda_available():
    import cupy as cp

# %% ../nbs/API/dl.ipynb 5
def _ort_session(
    path:str, # path to the model in onnx format
    cuda:bool=False, # if use cuda or not
):
    if cuda:
        import cupy as cp
        providers = [("CUDAExecutionProvider", {"device_id": cp.cuda.runtime.getDevice()}),'CPUExecutionProvider']
    else:
        providers = ['CPUExecutionProvider']
    ort_session = onnxruntime.InferenceSession(path, providers=providers)
    return ort_session

# %% ../nbs/API/dl.ipynb 6
@ngpjit
def _pre_infer_n2f_numba(intf):
    nlines, width = intf.shape
    out = np.empty((1,2,nlines,width),dtype=np.float32)
    mask = np.empty((nlines,width),dtype=np.bool_)
    for i in prange(nlines):
        for j in prange(width):
            intf_i_j = intf[i,j]
            if math.isnan(intf_i_j.real):
                mask[i,j] = True
                random_phase = random.uniform(-math.pi,math.pi)
                out[0,0,i,j] = math.cos(random_phase)
                out[0,1,i,j] = math.sin(random_phase)
            else:
                mask[i,j] = False
                amp = abs(intf_i_j)
                out[0,0,i,j] = intf_i_j.real/amp
                out[0,1,i,j] = intf_i_j.imag/amp
    return out, mask

# %% ../nbs/API/dl.ipynb 7
if is_cuda_available():
    _pre_infer_n2f_kernel = cp.ElementwiseKernel(
            'raw T intf, int32 nlines, int32 width',
            'raw float32 out, raw bool mask',
            '''
            int npixels = nlines*width;
            if (i >= npixels) return;

            //int az = i/width;
            //int r = i-az*width;
            T intf_i = intf[i];
            if (isnan(intf_i.real())){
                mask[i] = true;
            }
            else{
                mask[i] = false;
                float amp = abs(intf_i);
                out[i] = intf_i.real()/amp;
                out[npixels+i] = intf_i.imag()/amp;
            }
            ''',
            #preamble = '#include "curand.h"',
            # I do not find an easy way to generate random number with cupy kernel
            name = 'pre_infer_n2f_kernel',no_return=True)

# %% ../nbs/API/dl.ipynb 8
if is_cuda_available():
    def _pre_infer_n2f_cp(intf):
        nlines, width = intf.shape
        out = cp.empty((1,2,nlines,width),dtype=cp.float32)
        mask = cp.empty((nlines,width),dtype=cp.bool_)
        _pre_infer_n2f_kernel(intf,cp.int32(nlines),cp.int32(width),out,mask,size=nlines*width,block_size=128)
        nan_pos = cp.where(mask)
        random_phase = cp.random.uniform(-cp.pi,cp.pi,len(nan_pos[0]))
        out[0,0,nan_pos[0],nan_pos[1]] = cp.cos(random_phase)
        out[0,1,nan_pos[0],nan_pos[1]] = cp.sin(random_phase)
        return out, mask

# %% ../nbs/API/dl.ipynb 12
@ngpjit
def _after_infer_n2f_numba(
    infer_out,
    mask,
):
    nlines, width = mask.shape
    out = np.empty((nlines,width),dtype=np.complex64)
    for i in prange(nlines):
        for j in prange(width):
            if mask[i,j]:
                out[i,j] = np.nan+1j*np.nan
            else:
                out[i,j] = infer_out[0,0,i,j]+1j*infer_out[0,1,i,j]
    return out

# %% ../nbs/API/dl.ipynb 13
if is_cuda_available():
    _after_infer_n2f_kernel = cp.ElementwiseKernel(
            'raw float32 infer_out, raw bool mask, int32 nlines, int32 width',
            'raw T out',
            '''
            int npixels = nlines*width;
            if (i >= npixels) return;

            if (mask[i]){
                out[i] = T(CUDART_NAN,CUDART_NAN);
            }
            else{
                out[i] = T(infer_out[i],infer_out[npixels+i]);
            }
            ''',
            preamble = '#include <cupy/math_constants.h>',
            name = 'after_infer_n2f_kernel',no_return=True)

# %% ../nbs/API/dl.ipynb 14
if is_cuda_available():
    def _after_infer_n2f_cp(infer_out,mask):
        nlines, width = mask.shape
        out = cp.empty((nlines,width),dtype=cp.complex64)
        _after_infer_n2f_kernel(infer_out,mask,cp.int32(nlines),cp.int32(width),out,size=nlines*width,block_size=128)
        return out

# %% ../nbs/API/dl.ipynb 19
def _infer_n2f_cpu(
    intf,
    session,
):
    input_intf, mask = _pre_infer_n2f_numba(intf)
    infer_out = session.run([session.get_outputs()[0].name,],{session.get_inputs()[0].name: input_intf})[0]
    return _after_infer_n2f_numba(infer_out,mask)

# %% ../nbs/API/dl.ipynb 20
def _infer_n2f_gpu(
    intf,
    session,
):
    input_intf, mask = _pre_infer_n2f_cp(intf)
    input_intf = cp.ascontiguousarray(input_intf)
    out = cp.ascontiguousarray(cp.empty_like(input_intf))
    io_binding = session.io_binding()
    io_binding.bind_input(
            name=session.get_inputs()[0].name,
            device_type='cuda',
            device_id=input_intf.device.id,
            element_type=input_intf.dtype,
            shape=list(input_intf.shape),
            buffer_ptr=input_intf.data.ptr,
    )
    io_binding.bind_output(
            name=session.get_outputs()[0].name,
            device_type='cuda',
            device_id=out.device.id,
            element_type=out.dtype,
            shape=list(out.shape),
            buffer_ptr=out.data.ptr,
    )
    io_binding.synchronize_inputs()
    session.run_with_iobinding(io_binding)
    return _after_infer_n2f_cp(out,mask)

# %% ../nbs/API/dl.ipynb 21
def n2f(
    intf:np.ndarray, # interferogram, 2d np.complex64 or cp.complex64
    chunks:tuple=None, # chunksize, intf.shape by default 
    depths:tuple=(0,0), # width of the boundary
    model:str=None, # path to the model in onnx format, use the model comes with this package by default
):
    xp = mr.utils_.get_array_module(intf)
    if model is None:
        model = importlib.resources.files('moraine')/'dl_model/n2f.onnx'
    shape = intf.shape
    if chunks is None: chunks = shape
    in_slices, out_slices, map_slices = chunkwise_slicing_mapping(shape,chunks,depths)
    out = xp.empty_like(intf)

    if xp is np:
        session = _ort_session(model,cuda=False)
        for in_slice, out_slice, map_slice in zip(in_slices, out_slices, map_slices):
            out[out_slice] = _infer_n2f_cpu(intf[in_slice],session)[map_slice]
    else:
        session = _ort_session(model,cuda=True)
        for in_slice, out_slice, map_slice in zip(in_slices, out_slices, map_slices):
            out[out_slice] = _infer_n2f_gpu(intf[in_slice],session)[map_slice]

    return out

# %% ../nbs/API/dl.ipynb 31
def _n2f_np_in_gpu(
    intf:np.ndarray, # interferogram, 2d np.complex64 or cp.complex64
    chunks:tuple=None, # chunksize, intf.shape by default 
    depths:tuple=(0,0), # width of the boundary
    model:str=None, # path to the model in onnx format, use the model comes with this package by default
):
    '''compare with n2f, input and output are np.ndarray but use gpu for inference'''
    if model is None:
        model = importlib.resources.files('moraine')/'dl_model/n2f.onnx'
    shape = intf.shape
    if chunks is None: chunks = shape
    in_slices, out_slices, map_slices = chunkwise_slicing_mapping(shape,chunks,depths)
    out = np.empty_like(intf)

    session = _ort_session(model,cuda=True)
    for in_slice, out_slice, map_slice in zip(in_slices, out_slices, map_slices):
        out[out_slice] = _infer_n2f_gpu(cp.asarray(intf[in_slice]),session)[map_slice].get()
    return out
