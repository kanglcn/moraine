# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/CLI/co.ipynb.

# %% auto 0
__all__ = ['emperical_co_pc']

# %% ../../nbs/CLI/co.ipynb 4
import logging
import time

import zarr
import numpy as np
import math

import dask
from dask import array as da
from dask import delayed
from dask.distributed import Client, LocalCluster, progress
from ..utils_ import is_cuda_available, get_array_module
if is_cuda_available():
    import cupy as cp
    from dask_cuda import LocalCUDACluster
    from rmm.allocators.cupy import rmm_cupy_allocator
import moraine as mr
from .logging import mc_logger
from . import dask_from_zarr, dask_to_zarr

# %% ../../nbs/CLI/co.ipynb 5
@mc_logger
def emperical_co_pc(
    rslc:str, # input: rslc stack, shape (nlines, width, nimages)
    is_shp:str, # input: bool array indicating the SHPs of pc, shape (n_points, az_win, r_win)
    gix:str, # input: bool array indicating pc, shape (2, n_points)
    coh:str, # output: complex coherence matrix for pc
    tnet:str=None, # input: temporal network
    az_chunks:int=None, # processing azimuth chunk size, optional. Default: the azimuth chunk size in rslc stack
    chunks:int=None, # chunk size of output zarr dataset, optional. Default: same as is_shp
    cuda:bool=False, # if use cuda for processing, false by default
    processes=None, # use process for dask worker over thread, the default is False for cpu, only applied if cuda==False
    n_workers=None, # number of dask worker, the default is 1 for cpu, number of GPU for cuda
    threads_per_worker=None, # number of threads per dask worker, the default is 2 for cpu, only applied if cuda==False
    rmm_pool_size=0.9, # set the rmm pool size, only applied when cuda==True
    **dask_cluster_arg, # other dask local/cudalocal cluster args
):
    '''estimate emperical coherence matrix on point cloud data.
    Due to the data locality problem. r_chunk_size for the processing have to be one.
    '''
    rslc_path = rslc
    is_shp_path = is_shp
    gix_path = gix
    coh_path = coh
    logger = logging.getLogger(__name__)

    rslc_zarr = zarr.open(rslc_path,mode='r')
    logger.zarr_info(rslc_path, rslc_zarr)
    assert rslc_zarr.ndim == 3, "rslc dimentation is not 3."
    nlines, width, nimage = rslc_zarr.shape

    is_shp_zarr = zarr.open(is_shp_path,mode='r')
    logger.zarr_info(is_shp_path, is_shp_zarr)
    assert is_shp_zarr.ndim == 3, "is_shp dimentation is not 3."

    gix_zarr = zarr.open(gix_path,mode='r')
    logger.zarr_info(gix_path, gix_zarr)
    assert gix_zarr.ndim == 2, "gix dimentation is not 2."
    logger.info('loading gix into memory.')
    gix = zarr.open(gix_path,mode='r')[:]

    az_win, r_win = is_shp_zarr.shape[1:]
    az_half_win = int((az_win-1)/2)
    r_half_win = int((r_win-1)/2)
    logger.info(f'''got azimuth window size and half azimuth window size
    from is_shp shape: {az_win}, {az_half_win}''')
    logger.info(f'''got range window size and half range window size
    from is_shp shape: {r_win}, {r_half_win}''')

    if az_chunks is None: az_chunks = rslc_zarr.chunks[0]
    logger.info('parallel processing azimuth chunk size: '+str(az_chunks))
    logger.info(f'parallel processing range chunk size: {width}')

    n_az_chunk = int(np.ceil(nlines/az_chunks))
    j_chunk_boundary = np.arange(n_az_chunk+1)*az_chunks; j_chunk_boundary[-1] = nlines
    point_boundary = np.searchsorted(gix[:,0],j_chunk_boundary)
    process_pc_chunk_size = np.diff(point_boundary)
    process_pc_chunk_size = tuple(process_pc_chunk_size.tolist())
    logger.info(f'number of point in each chunk: {process_pc_chunk_size}')

    if cuda:
        Cluster = LocalCUDACluster; cluster_args= {
            'n_workers':n_workers,
            'rmm_pool_size':rmm_pool_size}
        cluster_args.update(dask_cluster_arg)
        xp = cp
    else:
        if processes is None: processes = False
        if n_workers is None: n_workers = 1
        if threads_per_worker is None: threads_per_worker = 2
        Cluster = LocalCluster; cluster_args = {'processes':processes, 'n_workers':n_workers, 'threads_per_worker':threads_per_worker}
        cluster_args.update(dask_cluster_arg)
        xp = np

    if tnet is not None:
        tnet = mr.TempNet.load(tnet)
    else:
        tnet = mr.TempNet.from_bandwidth(nimage)
    image_pairs = tnet.image_pairs
    n_image_pairs = image_pairs.shape[0]
    
    logger.info('starting dask cluster.')
    with Cluster(**cluster_args) as cluster, Client(cluster) as client:
        logger.info('dask cluster started.')
        logger.dask_cluster_info(cluster)
        if cuda: client.run(cp.cuda.set_allocator, rmm_cupy_allocator)
        emperical_co_pc_delayed = delayed(mr.emperical_co_pc,pure=True,nout=1)

        cpu_is_shp = dask_from_zarr(is_shp_path,parallel_dims=(1,2))
        cpu_is_shp = cpu_is_shp.rechunk((process_pc_chunk_size,(az_win,),(r_win,)))
        #cpu_is_shp = da.from_zarr(is_shp_path,chunks=(process_pc_chunk_size,(az_win,),(r_win,)),inline_array=True)
        logger.darr_info('is_shp', cpu_is_shp)
        if cuda:
            is_shp = cpu_is_shp.map_blocks(cp.asarray)
        else:
            is_shp = cpu_is_shp
        is_shp_delayed = is_shp.to_delayed()
        is_shp_delayed = np.squeeze(is_shp_delayed,axis=(-2,-1))

        cpu_rslc = dask_from_zarr(rslc_path,parallel_dims=(1,2))
        cpu_rslc = cpu_rslc.rechunk((az_chunks,*rslc_zarr.shape[1:]))
        #cpu_rslc = da.from_zarr(rslc_path,chunks=(az_chunks,*rslc_zarr.shape[1:]),inline_array=True)
        logger.darr_info('rslc', cpu_rslc)
        depth = {0:az_half_win, 1:r_half_win, 2:0}; boundary = {0:'none',1:'none',2:'none'}
        cpu_rslc_overlap = da.overlap.overlap(cpu_rslc,depth=depth, boundary=boundary)
        logger.info('setting shared boundaries between rlsc chunks.')
        logger.darr_info('rslc_overlap', cpu_rslc_overlap)
        if cuda:
            rslc_overlap = cpu_rslc_overlap.map_blocks(cp.asarray)
        else:
            rslc_overlap = cpu_rslc_overlap
        rslc_overlap_delayed = np.squeeze(rslc_overlap.to_delayed(),axis=(-2,-1))

        coh_delayed = np.empty(n_az_chunk,dtype=object)
        logger.info(f'estimating coherence matrix.')
        for j in range(n_az_chunk):
            jstart = j*az_chunks; jend = jstart + az_chunks
            if jend>=nlines: jend = nlines
            gix_local_j = gix[point_boundary[j]:point_boundary[j+1],0]-jstart
            if j!= 0: gix_local_j += az_half_win
            gix_local_i = gix[point_boundary[j]:point_boundary[j+1],1]
            gix_local = np.stack((gix_local_j,gix_local_i),axis=-1)
            gix_local_delayed = da.from_array(gix_local)
            if cuda:
                gix_local_delayed = gix_local_delayed.map_blocks(cp.asarray)

            coh_delayed[j] = emperical_co_pc_delayed(rslc_overlap_delayed[j],gix_local_delayed,is_shp_delayed[j],image_pairs=image_pairs)
            coh_delayed[j] = da.from_delayed(coh_delayed[j],shape=(process_pc_chunk_size[j],n_image_pairs),meta=xp.array((),dtype=xp.complex64))

        coh = da.block(coh_delayed[...,None].tolist())
        if cuda:
            cpu_coh = coh.map_blocks(cp.asnumpy)
        else:
            cpu_coh = coh
        logger.info('get coherence matrix.'); logger.darr_info('coh', cpu_coh)

        if chunks is None: chunks = is_shp_zarr.chunks[0]
        logger.info(f'rechunking coh to chunk size (for saving with zarr): {chunks}')
        cpu_coh = cpu_coh.rechunk((chunks,cpu_coh.shape[1]))
        logger.darr_info('coh', cpu_coh)

        logger.info('saving coh.')
        _cpu_coh = dask_to_zarr(cpu_coh,coh_path,chunks=(chunks,1))
        #_cpu_coh = cpu_coh.to_zarr(coh_path,overwrite=True,compute=False)

        logger.info('computing graph setted. doing all the computing.')
        #This function is really slow just because the coherence is very big and rechunk and saving takes too much time.
        # I do not find any solution to it.
        futures = client.persist([_cpu_coh,])
        progress(futures,notebook=False)
        time.sleep(0.1)
        da.compute(futures)
        # pdb.set_trace()
        logger.info('computing finished.')
    logger.info('dask cluster closed.')
